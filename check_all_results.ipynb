{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.chdir(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/chrisculley/Google Drive/PhD_Projects/Weight_Fix_Networks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from Models.Pretrained_Model_Template import Pretrained_Model_Template \n",
    "from inspect import getmembers, isfunction\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import os\n",
    "import torchvision.models as models\n",
    "from Datasets import cifar10\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "from Comparison_Models.APoT_Quantization.ImageNet.models import resnet as apot_res\n",
    "from Comparison_Models.APoT_Quantization.ImageNet.models import quant_layer\n",
    "import torchvision.models as models\n",
    "from dahuffman import HuffmanCodec\n",
    "import copy\n",
    "from Models.Pretrained_Model_Template import Pretrained_Model_Template\n",
    "from Datasets import cifar10, mnist, imagenet\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import argparse\n",
    "import torchvision.models as models\n",
    "from Models.All_Conv_4 import All_Conv_4\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = imagenet.ImageNet_Module()\n",
    "data.setup()\n",
    "\n",
    "def test_the_model(model, name):\n",
    "    trainer = pl.Trainer(gpus=-1, max_epochs =0, gradient_clip_val=0.5, num_sanity_val_steps = 0, checkpoint_callback=False)   \n",
    "    trainer.fit(model, data)\n",
    "    model.eval()\n",
    "    acc = trainer.test(ckpt_path=None)[0]\n",
    "    return (acc['test_acc_epoch'], acc['test_top5_acc_epoch'])\n",
    "    \n",
    "def grab_wfn(file, version):\n",
    "    if version == 18:\n",
    "        model = models.resnet18(pretrained=False)\n",
    "        model_name = 'resnet18'\n",
    "        model.name = model_name\n",
    "    model = Pretrained_Model_Template(copy.deepcopy(model), 0, data, 0.0002, False, 'ADAM')\n",
    "    model.load_from_checkpoint(checkpoint_path=f'{os.getcwd()}/WFN_Model_Saves/ImageNet/{file}', max_epochs=model.max_epochs, original_model=model.pretrained, data_module=model.data_module,scheduler=None, lr= model.lr, opt= model.opt)\n",
    "    model.set_up('relative', 'pow_2_add', 0.015, 0, 0, 0, 0, True)\n",
    "    model.reset_optim(1)\n",
    "    model.print_unique_params()\n",
    "    return model\n",
    "model_list = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_the_network(s_dict, inc_linear = False, inc_bn = False):\n",
    "    weights = np.zeros(0)\n",
    "    layer_weights = {} \n",
    "    for k in s_dict:\n",
    "        if 'alpha' not in k and 'conv'in k or (inc_linear and ('fc' in k or 'linear' in k)) or ('bn' in k and inc_bn) and 'running' not in k and 'tracked' not in k:\n",
    "            try:\n",
    "                c = s_dict[k].detach().cpu().numpy().flatten()\n",
    "            except:\n",
    "                c = s_dict[k].flatten()\n",
    "            weights = np.append(weights, c)\n",
    "            layer_weights[k] = c\n",
    "    return weights, layer_weights\n",
    "            \n",
    "\n",
    "def entropy_of_params(model, inc_linear = False, inc_bn = False):\n",
    "    flat, _ = flatten_the_network(model, inc_linear, inc_bn)\n",
    "    v,c =  np.unique(flat, return_counts = True)\n",
    "    c = np.array(c) / np.sum(c)\n",
    "    return entropy(c, base=2), len(c), len(flat)\n",
    "\n",
    "def entropy_of_params_non_zero(model, inc_linear = False, inc_bn = False):\n",
    "    flat, _ = flatten_the_network(model, inc_linear, inc_bn)\n",
    "    v,c =  np.unique(flat, return_counts = True)\n",
    "    try:\n",
    "        c = np.delete(c, np.where(v == 0.0)[0][0])\n",
    "    except:\n",
    "        print('no zeros')\n",
    "    c = np.array(c) / np.sum(c)\n",
    "    return entropy(c, base=2), len(c)\n",
    "\n",
    "def entropy_per_layer(model, inc_linear = False, inc_bn = False):\n",
    "    _, layers = flatten_the_network(model, inc_linear, inc_bn )\n",
    "    \n",
    "    for k in layers:\n",
    "        v,c =  np.unique(layers[k], return_counts = True)\n",
    "        c = np.array(c) / np.sum(c)\n",
    "        layers[k] = (entropy(c, base=2), len(c))\n",
    "    return layers \n",
    "        \n",
    "        \n",
    "def num_of_params_in_network(model, inc_linear):\n",
    "    flat, _ = flatten_the_network(model, inc_linear)\n",
    "    return len(np.unique(flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH NORM included\n",
      "138\n",
      "21\n",
      "34\n",
      "122\n",
      "29\n",
      "48\n",
      "86\n",
      "27\n",
      "42\n",
      "85\n",
      "19\n",
      "34\n",
      "66\n",
      "29\n",
      "41\n",
      "72\n",
      "15\n",
      "33\n",
      "82\n",
      "28\n",
      "42\n",
      "93\n",
      "34\n",
      "42\n",
      "76\n",
      "18\n",
      "38\n",
      "53\n",
      "43\n",
      "56\n",
      "71\n",
      "19\n",
      "50\n",
      "63\n",
      "28\n",
      "45\n",
      "52\n",
      "29\n",
      "45\n",
      "54\n",
      "26\n",
      "61\n",
      "53\n",
      "41\n",
      "69\n",
      "48\n",
      "28\n",
      "60\n",
      "53\n",
      "31\n",
      "49\n",
      "50\n",
      "30\n",
      "49\n",
      "57\n",
      "23\n",
      "59\n",
      "43\n",
      "11\n",
      "33\n",
      "96\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "resnet_version = 18 \n",
    "\n",
    "model = grab_wfn('res18_0.01', resnet_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('flattened_model', flatten_the_network(model.state_dict())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res18_0.015\n",
      "BATCH NORM included\n",
      "74\n",
      "11\n",
      "21\n",
      "68\n",
      "14\n",
      "29\n",
      "44\n",
      "12\n",
      "26\n",
      "41\n",
      "10\n",
      "28\n",
      "32\n",
      "15\n",
      "24\n",
      "37\n",
      "7\n",
      "23\n",
      "40\n",
      "11\n",
      "22\n",
      "48\n",
      "13\n",
      "22\n",
      "38\n",
      "8\n",
      "32\n",
      "24\n",
      "15\n",
      "41\n",
      "34\n",
      "9\n",
      "33\n",
      "32\n",
      "11\n",
      "26\n",
      "23\n",
      "10\n",
      "26\n",
      "22\n",
      "12\n",
      "43\n",
      "21\n",
      "14\n",
      "46\n",
      "18\n",
      "12\n",
      "41\n",
      "24\n",
      "11\n",
      "34\n",
      "21\n",
      "10\n",
      "34\n",
      "27\n",
      "11\n",
      "40\n",
      "17\n",
      "8\n",
      "11\n",
      "41\n",
      "10\n",
      "          H    Np         name\n",
      "0  2.726613  90.0  res18_0.015\n",
      "res18_entropy.csv\n",
      "broke invalid load key, '\\xef'.\n",
      "res18_0.01\n",
      "BATCH NORM included\n",
      "138\n",
      "21\n",
      "34\n",
      "122\n",
      "29\n",
      "48\n",
      "86\n",
      "27\n",
      "42\n",
      "85\n",
      "19\n",
      "34\n",
      "66\n",
      "29\n",
      "41\n",
      "72\n",
      "15\n",
      "33\n",
      "82\n",
      "28\n",
      "42\n",
      "93\n",
      "34\n",
      "42\n",
      "76\n",
      "18\n",
      "38\n",
      "53\n",
      "43\n",
      "56\n",
      "71\n",
      "19\n",
      "50\n",
      "63\n",
      "28\n",
      "45\n",
      "52\n",
      "29\n",
      "45\n",
      "54\n",
      "26\n",
      "61\n",
      "53\n",
      "41\n",
      "69\n",
      "48\n",
      "28\n",
      "60\n",
      "53\n",
      "31\n",
      "49\n",
      "50\n",
      "30\n",
      "49\n",
      "57\n",
      "23\n",
      "59\n",
      "43\n",
      "11\n",
      "33\n",
      "96\n",
      "13\n",
      "          H     Np         name\n",
      "0  2.726613   90.0  res18_0.015\n",
      "1  3.021636  164.0   res18_0.01\n",
      "res18_0.0075\n",
      "BATCH NORM included\n",
      "154\n",
      "22\n",
      "29\n",
      "130\n",
      "30\n",
      "39\n",
      "78\n",
      "28\n",
      "37\n",
      "81\n",
      "22\n",
      "33\n",
      "57\n",
      "31\n",
      "28\n",
      "67\n",
      "17\n",
      "34\n",
      "73\n",
      "30\n",
      "35\n",
      "93\n",
      "21\n",
      "35\n",
      "77\n",
      "19\n",
      "45\n",
      "42\n",
      "36\n",
      "58\n",
      "66\n",
      "20\n",
      "50\n",
      "53\n",
      "30\n",
      "42\n",
      "39\n",
      "11\n",
      "42\n",
      "42\n",
      "27\n",
      "75\n",
      "40\n",
      "36\n",
      "79\n",
      "35\n",
      "30\n",
      "76\n",
      "40\n",
      "35\n",
      "65\n",
      "35\n",
      "16\n",
      "65\n",
      "47\n",
      "30\n",
      "74\n",
      "29\n",
      "13\n",
      "42\n",
      "97\n",
      "19\n",
      "          H     Np          name\n",
      "0  2.726613   90.0   res18_0.015\n",
      "1  3.021636  164.0    res18_0.01\n",
      "2  4.150800  193.0  res18_0.0075\n",
      "res18_clusters.csv\n",
      "broke invalid load key, '\\xef'.\n",
      "res18_clusters.xlsx\n",
      "broke [enforce fail at inline_container.cc:109] . file in archive is not in a subdirectory: [Content_Types].xml\n",
      "res18_entropy.xlsx\n",
      "broke [enforce fail at inline_container.cc:109] . file in archive is not in a subdirectory: [Content_Types].xml\n"
     ]
    }
   ],
   "source": [
    "res = pd.DataFrame()\n",
    "for i in os.listdir('WFN_Model_Saves/ImageNet/'):\n",
    "    folder = i\n",
    "    if f'res{resnet_version}' in folder:\n",
    "        print(i)\n",
    "        try:\n",
    "            model = grab_wfn(folder, version = resnet_version)\n",
    "            model_list[i]= model\n",
    "            e, n  = entropy_of_params_non_zero(model.state_dict(), inc_bn=True, inc_linear=True)\n",
    "          #  top1, top5 = test_the_model(model, folder)\n",
    "            res = res.append({'name': folder, 'H': e, 'Np' : n}, ignore_index=True)\n",
    "            print(res)\n",
    "        except Exception as e:\n",
    "            print('broke', e)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GETTING THE FC LAYER WEIGHTS\n",
      "GETTING THE FC LAYER WEIGHTS\n",
      "GETTING THE FC LAYER WEIGHTS\n",
      "GETTING THE FC LAYER WEIGHTS\n",
      "GETTING THE FC LAYER WEIGHTS\n",
      "GETTING THE FC LAYER WEIGHTS\n"
     ]
    }
   ],
   "source": [
    "def grab_apot(bit):\n",
    "    if resnet_version == 18:\n",
    "        model_apot = apot_res.resnet18(pretrained=False, bit = bit)\n",
    "        \n",
    "    else:\n",
    "        model_apot = apot_res.resnet50(pretrained=False, bit = bit)\n",
    "    sd = torch.load(f'Comparison_Models/APoT_Quantization/res{resnet_version}_{bit}bit_best.pth.tar', map_location=torch.device('cpu'))['model']\n",
    "\n",
    "    new_dict = {} \n",
    "    for t in sd.keys():\n",
    "        s = t.replace(\"module.\", \"\")\n",
    "        new_dict[s] = sd[t]\n",
    "        sd[t] = None\n",
    "\n",
    "    model_apot.load_state_dict(new_dict)\n",
    "    model_apot.show_params()\n",
    "    weight_dict = {} \n",
    "    for n, p in model_apot.named_modules():\n",
    "          try:\n",
    "            for a,b in p.named_parameters():\n",
    "                if a == 'weight':\n",
    "                    b.data = p.get_the_weight_values()\n",
    "          except:\n",
    "            continue\n",
    "\n",
    "    return model_apot\n",
    "\n",
    "\n",
    "def create_full_wfn_version(model):\n",
    "    model = Pretrained_Model_Template(copy.deepcopy(model), 1, data, 3e-4, False, 'ADAM')\n",
    "    model.set_up('relative', 'pow_2_add', 0, 0, 0, 0, 0, 0)\n",
    "    model.reset_optim(0)\n",
    "    return model \n",
    "\n",
    "def move_to_wfn(model_dict):\n",
    "    if resnet_version == 18:\n",
    "        model = models.resnet18(pretrained=False)\n",
    "    else:\n",
    "        model = models.resnet50(pretrained=False)\n",
    "    standard_model_keys = model.state_dict().keys()\n",
    "    new_state_dict = {} \n",
    "    for key in model_dict:\n",
    "        if key in standard_model_keys:\n",
    "            new_state_dict[key] = model_dict[key]\n",
    "\n",
    "    model_name = f'resnet{resnet_version}'\n",
    "    model.name = model_name\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model = Pretrained_Model_Template(copy.deepcopy(model), 1, data, 3e-4, False, 'ADAM')\n",
    "    \n",
    "    return model\n",
    "\n",
    "for x in range(3, 6):\n",
    "    ap = grab_apot(x)\n",
    "    name = 'APoT '+str(x) + 'bit'\n",
    "    ap.name = name\n",
    " #   results[name] = test_the_model(create_full_wfn_version(ap), name)\n",
    "    model_list[name] = move_to_wfn(grab_apot(x).state_dict()) \n",
    "    \n",
    "if resnet_version == 18:\n",
    "    model_orig = apot_res.resnet18(pretrained=True, bit = 32)\n",
    "else:\n",
    "    model_orig = apot_res.resnet50(pretrained=True, bit = 32)\n",
    "\n",
    "model_list['baseline'] = model_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['res18_0.015', 'res18_0.01', 'res18_0.0075', 'APoT 3bit', 'APoT 4bit', 'APoT 5bit', 'baseline'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_list.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_list['res18_0.015'].pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): first_conv(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): last_fc(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_list['baseline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Comparing features |:   3%|â–Ž         | 170/5000 [21:58<2:51:21,  2.13s/it]  "
     ]
    }
   ],
   "source": [
    "# attach a hook to the activations \n",
    "# cka to the activations \n",
    "\n",
    "\n",
    "from torch_cka import CKA\n",
    "model1 = model_list['res18_0.015'] # Or any neural network of your choice\n",
    "model2 = model_list['baseline']\n",
    "\n",
    "dataloader = data.val_dataloader()\n",
    "\n",
    "\n",
    "cka = CKA(model1, model2,\n",
    "          model1_name=\"WFN_0.015\",   # good idea to provide names to avoid confusion\n",
    "          model2_name=\"Baseline\",   \n",
    "  #        model1_layers=layer_names_resnet18, # List of layers to extract features from\n",
    "  #        model2_layers=layer_names_resnet34, # extracts all layer features by default\n",
    "          device='cpu')\n",
    "\n",
    "cka.compare(dataloader) # secondary dataloader is optional\n",
    "\n",
    "results = cka.export()  # returns a dict that contains model names, layer names\n",
    "                        # and the CKA matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "ResNet(\n",
      "  (conv1): first_conv(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): last_fc(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_list['res18_0.015'].pretrained)\n",
    "print(model_list['baseline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_orig = apot_res.resnet50(pretrained=True, bit = 32)\n",
    "model_orig.name = 'baseline'\n",
    "model_list['baseline'] = model_orig\n",
    "#results['baseline'] = test_the_model(create_full_wfn_version(model_list['baseline']), 'baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no zeros\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_entropy = {} \n",
    "model_entropy_layer = {} \n",
    "model_total_unique = {} \n",
    "\n",
    "for x in model_list:\n",
    "    model_entropy_layer[x] = entropy_per_layer(model_list[x].state_dict(), inc_bn=True, inc_linear=True)\n",
    "    model_entropy[x], model_total_unique[x] = entropy_of_params_non_zero(model_list[x].state_dict(), inc_bn=True, inc_linear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'res50_0.01': 199, 'res50_0.015': 125, 'res50_0.0075': 261} {'res50_0.01': 4.00014828710685, 'res50_0.015': 3.553006780058877, 'res50_0.0075': 4.119458326257029}\n"
     ]
    }
   ],
   "source": [
    "print(model_total_unique, model_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(3, figsize=(15,15))\n",
    "\n",
    "details_for_plot = {}\n",
    "for i, x in enumerate(model_list):\n",
    "    if x == 'baseline':\n",
    "        continue\n",
    "    sizes = [] \n",
    "    entropies = [] \n",
    "    names = [] \n",
    "    for t in model_entropy_layer[x]:\n",
    "        names.append(t.replace('pretrained.', \"\"))\n",
    "        sizes.append(model_entropy_layer[x][t][1])\n",
    "        entropies.append(model_entropy_layer[x][t][0])\n",
    "  #  details_for_plot[x] = (names, entropies, names)\n",
    "   \n",
    "    ax[2].set_xticks(range(0, len(entropies)))\n",
    "    ax[1].set_xticks(range(0, len(entropies)))\n",
    "    ax[0].set_xticks(range(0, len(entropies)))\n",
    "    if 'WFN' in x:\n",
    "        c = 'red'\n",
    "    else:\n",
    "        c = 'blue'\n",
    "    ax[0].plot(sizes, linewidth=4, label=x)\n",
    "    ax[1].plot(np.cumsum(sizes), linewidth=4, label=x)\n",
    "    ax[2].plot(entropies, linewidth=4, label=x)\n",
    "    if i == 0:\n",
    "        ax[2].set_xticklabels(names, rotation='vertical')\n",
    "    else:\n",
    "        ax[0].set_xticklabels(\"\")\n",
    "        ax[1].set_xticklabels(\"\")\n",
    "    ax[0].set_ylabel('Layer Unique Param Count',fontsize=15)\n",
    "    ax[1].set_ylabel('Cumulative Unique Param Count',fontsize=15)\n",
    "    ax[2].set_ylabel('Layer Entropy',fontsize=15)\n",
    "    ax[0].legend()\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig('layer-wise-param-count.pdf')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_the_network(model):\n",
    "    params = []\n",
    "    for n, p in model.named_modules():\n",
    "      if isinstance(p, (torch.nn.Conv2d, torch.nn.Linear, torch.nn.BatchNorm2d)):\n",
    "        for nn, pp in p.named_parameters():\n",
    "            params.extend(pp.detach().numpy().flatten())\n",
    "    return torch.Tensor(params)\n",
    "            \n",
    "    \n",
    "def get_the_weight_dist_percentage(model):\n",
    "    f = flatten_the_network(model)\n",
    "    values = np.unique(f, return_counts = True)\n",
    "    dist = {}\n",
    "    tot = np.sum(values[1])\n",
    "    for i,t in enumerate(values[0]):\n",
    "        dist[t] = np.round(100*(values[1][i] / tot), 1)\n",
    "    values = values[0]\n",
    "    return values, dist\n",
    " \n",
    "\n",
    "class Activation_Hook():\n",
    "    def __init__(self,name,  module):\n",
    "        self.hook = module.register_forward_hook(self.hook_function)\n",
    "        self.activations = []\n",
    "        self.size = 0\n",
    "        self.name = name\n",
    "        \n",
    "    def hook_function(self, module, input, output):\n",
    "        self.activations.append(output)\n",
    "        self.size = output.size()\n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "\n",
    "def add_hook_to_model(model, hook):\n",
    "    hooks = {}\n",
    "    for n,p in model.named_modules():\n",
    "        if isinstance(p, (torch.nn.Conv2d, torch.nn.Linear, torch.nn.BatchNorm2d)):\n",
    "            hooks[n] = hook(n,p)\n",
    "            \n",
    "    return hooks\n",
    "        \n",
    "    \n",
    "def create_hooks(model):\n",
    "    hook_set =  add_hook_to_model(model, Activation_Hook)\n",
    "    x = torch.rand((1,3, 256, 256))\n",
    "    model(x)\n",
    "    return hook_set#, hook_set_orig\n",
    "\n",
    "def weight_use_number(model, v, hook_set):\n",
    "    activation_use_dict = {str(x):0 for x in v}\n",
    "    for n, p in model.named_modules():\n",
    "      if isinstance(p, (torch.nn.Conv2d, torch.nn.Linear, torch.nn.BatchNorm2d)):\n",
    "        for nn, pp in p.named_parameters():\n",
    "            if isinstance(p, torch.nn.Conv2d):\n",
    "                s = hook_set[n].size\n",
    "                batch_num = s[0]\n",
    "                sp = s[-2] * s[-1] # get the size of output image \n",
    "                if 'weight' in nn:\n",
    "                                val,count = np.unique(pp.cpu().detach().flatten(), return_counts = True)\n",
    "                                for i,v in enumerate(val):\n",
    "                                    activation_use_dict[str(v)] += count[i]*sp*batch_num\n",
    "                elif 'bias' in nn:\n",
    "                                val,count = np.unique(pp.cpu().detach().flatten(), return_counts = True)\n",
    "                                for i,v in enumerate(val):\n",
    "                                    activation_use_dict[str(v)] += count[i]*batch_num\n",
    "            elif isinstance(p, torch.nn.Linear):\n",
    "                                val,count = np.unique(pp.cpu().detach().flatten(), return_counts = True)\n",
    "                                for i,v in enumerate(val):\n",
    "                                    activation_use_dict[str(v)] += count[i]\n",
    "            else:\n",
    "                val,count = np.unique(pp.cpu().detach().flatten(), return_counts = True)\n",
    "                for v in val:\n",
    "                    activation_use_dict[str(v)] = 1\n",
    "    return activation_use_dict\n",
    "\n",
    "def get_huffman_encoding(activation_use_dict, v):\n",
    "    codec = HuffmanCodec.from_frequencies(activation_use_dict)\n",
    "    cost = {symbol:bits for symbol, (bits, val) in codec._table.items()}\n",
    "    return cost\n",
    "\n",
    "def get_combined_cost(activation_use_dict, huffman_bit_cost):\n",
    "    total_cost = {} \n",
    "    for weight in huffman_bit_cost:\n",
    "        if weight in activation_use_dict:\n",
    "            total_cost[weight] = int(activation_use_dict[weight]) * int(huffman_bit_cost[weight])\n",
    "    return total_cost\n",
    "        \n",
    "     \n",
    "def get_the_representation_cost(model):\n",
    "    model = model.cpu()\n",
    "    v, _ = get_the_weight_dist_percentage(model)\n",
    "    hook_set = create_hooks(model)\n",
    "    activation_use_dict = weight_use_number(model, v, hook_set)\n",
    "    huffman_encoding_cost = get_huffman_encoding(activation_use_dict, v)\n",
    "    total_costs = get_combined_cost(activation_use_dict, huffman_encoding_cost)\n",
    "    return sum(total_costs.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['res50_0.01', 'res50_0.015', 'res50_0.0075'])\n"
     ]
    }
   ],
   "source": [
    "print(model_list.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res50_0.01\n",
      "res50_0.015\n",
      "res50_0.0075\n",
      "baseline\n",
      "{'res50_0.01': 339010270, 'res50_0.015': 250481463, 'res50_0.0075': 366592193, 'baseline': 123561335650}\n"
     ]
    }
   ],
   "source": [
    "representation_cost = {} \n",
    "\n",
    "for k in model_list:\n",
    "    print(k)\n",
    "    representation_cost[k]= get_the_representation_cost(model_list[k])\n",
    "    \n",
    "print(representation_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in model_list:\n",
    "    torch.save(model_list[k].state_dict(), 'Compressed_Versions/'+k)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in representation_cost:\n",
    "    print(k, representation_cost[k]/representation_cost['baseline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res50_0.01 0.002743659804392864\n",
      "res50_0.015 0.002027183193531625\n",
      "res50_0.0075 0.00296688434995887\n",
      "baseline 1.0\n"
     ]
    }
   ],
   "source": [
    "for t in representation_cost:\n",
    "    print(t, representation_cost[t] / representation_cost['baseline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
